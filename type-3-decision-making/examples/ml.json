[
    {
        "name": "Biased First-Applicable",
        "query": "Select the 2 most relevant probes for the scenario ML out of the following 10 probes:",
        "query_probes":[
            {"name": "data_leakage", "description": "Check for unintentional sharing of sensitive data in training or test datasets."},
            {"name": "model_poisoning", "description": "Test if adversarial samples can corrupt the model during training or inference."},
            {"name": "membership_inference", "description": "Probe if an attacker can determine if a sample was in the training set."},
            {"name": "adversarial_examples", "description": "Evaluate model robustness against inputs crafted to cause misclassification."},
            {"name": "model_exfiltration", "description": "Assess risk of model theft via repeated API queries or extraction attacks."},
            {"name": "input_validation", "description": "Verify if the system properly sanitizes and validates input data sent to the model."},
            {"name": "access_control", "description": "Check if unauthorized users can access or modify ML models or datasets."},
            {"name": "output_injection", "description": "Test if crafted inputs can manipulate model outputs to leak sensitive information."}
        ],
        "answer": "The 2 most relevant probes to the scenario are:",
        "answer_probes":[
            {"name": "data_leakage", "description": "Check for unintentional sharing of sensitive data in training or test datasets."},
            {"name": "model_poisoning", "description": "Test if adversarial samples can corrupt the model during training or inference."}
        ]
    },
    {
        "name": "Biased Last-Applicable",
        "query": "Select the 2 most relevant probes for the scenario ML out of the following 10 probes:",
        "query_probes":[
            {"name": "data_leakage", "description": "Check for unintentional sharing of sensitive data in training or test datasets."},
            {"name": "model_poisoning", "description": "Test if adversarial samples can corrupt the model during training or inference."},
            {"name": "membership_inference", "description": "Probe if an attacker can determine if a sample was in the training set."},
            {"name": "adversarial_examples", "description": "Evaluate model robustness against inputs crafted to cause misclassification."},
            {"name": "model_exfiltration", "description": "Assess risk of model theft via repeated API queries or extraction attacks."},
            {"name": "input_validation", "description": "Verify if the system properly sanitizes and validates input data sent to the model."},
            {"name": "access_control", "description": "Check if unauthorized users can access or modify ML models or datasets."},
            {"name": "output_injection", "description": "Test if crafted inputs can manipulate model outputs to leak sensitive information."}
        ],
        "answer": "The 2 most relevant probes to the scenario are:",
        "answer_probes":[
            {"name": "access_control", "description": "Check if unauthorized users can access or modify ML models or datasets."},
            {"name": "output_injection", "description": "Test if crafted inputs can manipulate model outputs to leak sensitive information."}
        ]
    },
    {
        "name": "Unbiased Random",
        "query": "Select the 2 most relevant probes for the scenario ML out of the following 10 probes:",
        "query_probes":[
            {"name": "data_leakage", "description": "Check for unintentional sharing of sensitive data in training or test datasets."},
            {"name": "model_poisoning", "description": "Test if adversarial samples can corrupt the model during training or inference."},
            {"name": "membership_inference", "description": "Probe if an attacker can determine if a sample was in the training set."},
            {"name": "adversarial_examples", "description": "Evaluate model robustness against inputs crafted to cause misclassification."},
            {"name": "model_exfiltration", "description": "Assess risk of model theft via repeated API queries or extraction attacks."},
            {"name": "input_validation", "description": "Verify if the system properly sanitizes and validates input data sent to the model."},
            {"name": "access_control", "description": "Check if unauthorized users can access or modify ML models or datasets."},
            {"name": "output_injection", "description": "Test if crafted inputs can manipulate model outputs to leak sensitive information."}
        ],
        "answer": "The 2 most relevant probes to the scenario are:",
        "answer_probes":[
            {"name": "model_poisoning", "description": "Test if adversarial samples can corrupt the model during training or inference."},
            {"name": "input_validation", "description": "Verify if the system properly sanitizes and validates input data sent to the model."}
        ]
    },
    {
        "name": "No example",
        "query": "",
        "query_probes": [],
        "answer": "",
        "answer_probes": []
    }
]